"""Executes Diff&Diff model for each test group"""
# coding: utf-8

# # Execute Diff&Diff Model
# **Created by** : [Sarath Gadde](mailto:Sarath.Gadde@rb.com) (Artefact) <br>
# **Automated by** : [Jo√£o Arienti](mailto:joao.arienti@rb.com) <br>
# **Creation date** : 02-Dec-2020 <br>
# **Version** : 1.2 <br>
# **Notebook Objective** : This notebook is responsible for executing the Diff&Diff model. <br>
# **Brief Changelog** :
# * **05-Jan-2021** - Inclusion in the code of the logic to run this notebook or not taking into consideration the models that are set to execute in the config. file.
# * **29-Jan-2021** - Read config details from UI instead of config file.

# ## Import Libraries
import argparse
from datetime import date, datetime, timedelta
import json
import os
import re
import sys
import traceback
from urllib.parse import urlparse
from typing import Tuple

import ast
import numpy as np
import pandas as pd
from statistics import stdev

import google.auth
from google.cloud import storage
from google.cloud import bigquery
from google.cloud.exceptions import NotFound

from mroi.config import PROJECT_ID, EXECUTION_BASE_PATH, json_config
from mroi.data_models.inputs import MROIColumns
from mroi.notification import default_notification_handler, default_jobstatus_handler
from mroi.utils import gcs_join, GCSUrl
from mroi.logging import getLogger


# ## Exceptions
class ColumnHeadersError(Exception):
    ...

class DateFormatError(Exception):
    ...

class TooManyDateNullError(Exception):
    ...

class CampaignInfoError(Exception):
    ...


# ## Diff&Diff Class
class DiffDiff:
    """
    Class to execute Diff&Diff model for each test group

    Args:
        project_id (str): current project id on GCP
        bucket (str): GCS bucket to upload results to
        input_file_path (str): full path to input file on gcs
        config (dict): config json generated by UI
        model_config (dict): model config
        gcs_output_path (str): full path to output folder on gcs
        test_group (int): test group/cell number starting from 0
    """
    def __init__(self, project_id: str, bucket: str, input_file_path: str, config: dict, model_config: dict, gcs_output_path: str, test_group: int):
        self.logger = getLogger(self.__class__.__name__)
        
        self.PROJECT_ID = project_id
        self.input_file_path = input_file_path
        self.CONFIG = config
        self.MODEL_CONFIG = model_config
        self.gcs_output_path = gcs_output_path
        self.test_group = test_group
        
        self.upload_bucket_name = bucket
        self.output_file_url = GCSUrl(f'{gcs_output_path}/output_DIFF&DIFF_{self.test_group}.csv')
        self.storage_client = storage.Client()
        self.dataframe = pd.read_csv(self.input_file_path)

        self.VOL_COL = [col for col in self.dataframe.columns if self.MODEL_CONFIG['volume_field'] in col][0]
        
    def get_config(self):
        """
        Reads campaign config file and loads pre, post dates and reads config file for test, control region variables

        Raises:
            CampaignInfoError: Raised when more than 1 campaign found in model config
        """
        def align_dates(data_weekday: int, date_to_align: datetime.date):
            """
            Aligns input dates to data for weekly granularity
    
            Args:
                data_weekday (int): weekday of data
                date_to_align (datetime.date): input config date to convert using weekday
            Returns:
                datetime.date: input config date shifted to align with data
            """
            if data_weekday == date_to_align.weekday():
                return_date = date_to_align
            else:
                if data_weekday - date_to_align.weekday() > 0:
                    return_date = date_to_align - timedelta(days=7-(data_weekday-date_to_align.weekday()))
                else:
                    return_date = date_to_align - timedelta(days=(date_to_align.weekday()-data_weekday))
            return return_date
            
        test_control_regions = self.MODEL_CONFIG['region_granularity']['test_control_regions']
        # Retrieve test, control regions from config
        self.control_regions = [test_region.upper() for test_region in test_control_regions['control']["0"]["regions"]]
        self.test_regions = [test_region.upper() for test_region in test_control_regions['test'][str(self.test_group)]["regions"]]
        campaigns = test_control_regions['test'][str(self.test_group)]['campaigns']
         # check if monthly or weekly
        if any(self.dataframe.groupby(by=[self.dataframe[MROIColumns.date].dt.month, self.dataframe[MROIColumns.date].dt.year]).count().iloc[:, 0].values > 1): # weekly
            freq = 'weekly'
            data_weekday = self.dataframe[MROIColumns.date][0].weekday()
        else: # monthly
            freq = 'monthly'
        if len(campaigns) == 1:
            campaign_start_date = datetime.strptime(campaigns[0]['campaign_start_date'], '%Y-%m-%d').date()
            campaign_end_date = datetime.strptime(campaigns[0]['campaign_end_date'], '%Y-%m-%d').date()
            today_date = datetime.now().date()
            if freq == 'weekly':
                campaign_start = align_dates(data_weekday, campaign_start_date)
                campaign_end = align_dates(data_weekday, campaign_end_date)
                today = align_dates(data_weekday, today_date)
                # Check if campaign end date is after today
            else:
                campaign_start = campaign_start_date - timedelta(days=campaign_start_date.day - 1)
                campaign_end = campaign_end_date - timedelta(days=campaign_end_date.day - 1)
                # Check if campaign end date is after today
                today = today_date - timedelta(days=today_date.day - 1)
            if today < campaign_end:
                self.POST_END = pd.to_datetime(today)
            else:
                self.POST_END = pd.to_datetime(campaign_end)
            self.POST_BEGIN = pd.to_datetime(campaign_start)
            # get campaign duration upto today if it extends into the future
            campaign_duration = self.POST_END - self.POST_BEGIN
            try:
                pre_campaign_start_date = datetime.strptime(campaigns[0]['pre_campaign_start_date'], '%Y-%m-%d').date()
                pre_campaign_end_date = datetime.strptime(campaigns[0]['pre_campaign_end_date'], '%Y-%m-%d').date()
                if freq == 'weekly':
                    pre_campaign_start = align_dates(data_weekday, pre_campaign_start_date)
                    pre_campaign_end = align_dates(data_weekday, pre_campaign_end_date)
                else:
                    pre_campaign_start = pre_campaign_start_date - timedelta(days=pre_campaign_start_date.day - 1)
                    pre_campaign_end = pre_campaign_end_date - timedelta(days=pre_campaign_end_date.day - 1)  
                self.logger.info("Using the provided pre-campaign start/end dates.")
                # Calculate pre duration equivalent to campaign duration
                self.PRE_BEGIN = pd.to_datetime(pre_campaign_start)
                self.PRE_END = pd.to_datetime(pre_campaign_end)
                if campaign_duration != (self.PRE_END - self.PRE_BEGIN):
                    self.logger.warn("Duration does not match for pre-campaign and campaign periods. Using the campaign duration with pre-campaign start date to identify pre-campaign end date.")
                    self.PRE_END = self.PRE_BEGIN + campaign_duration
                self.logger.info(f"Campaign start date: {self.POST_BEGIN}")
                self.logger.info(f"Campaign end date: {self.POST_END}")
                self.logger.info(f"Pre-Campaign start date: {self.PRE_BEGIN}")
                self.logger.info(f"Pre-Campaign end date: {self.PRE_END}")
            except:
                raise CampaignInfoError(f'Pre-campaign start/end dates not provided.')
        else:
            raise CampaignInfoError(f'Found more than one campaign entry for Diff and Diff for test cell {self.test_group}. Script takes only one campaign entry to calculate Diff and Diff.')
        
    def check_columns_headers(self):
        """
        Test if required column headers exist.

        Raises:
            ColumnHeadersError: Raised when column headers don't match expected column headers
        """
        # List of sorted columns
        sorted_columns = sorted(list(self.dataframe.columns))
        self.logger.info('Checking if columns in data match the requirements.')
        # Check if columns are MROIColumns.date, MROIColumns.region, self.VOL_COL
        if sorted_columns != sorted([MROIColumns.date, MROIColumns.region, self.VOL_COL]):
            self.logger.error(
                f'''Invalid column headers in input file(s). 
                Require the following columns: {MROIColumns.date}, {MROIColumns.region}, {self.VOL_COL}.
                The input file has {sorted_columns}''')
            raise ColumnHeadersError('Invalid column headers in input file. Possibly due to Data corruption/change in Schema.')
        self.logger.info('Column check passed.')

    def check_date_format(self):
        """
        Test if date format is 'yyyy-mm-dd'.

        Raises:
            DateFormatError: Raised when date format is in yyyy-mm-dd format
        """
        try:
            self.logger.info('Checking date format.')
            # Try setting week_id format to '%Y-%m-%d' which is expected
            self.dataframe[MROIColumns.date] = pd.to_datetime(self.dataframe[MROIColumns.date], format='%Y-%m-%d', errors='raise')
        except Exception as e:
            raise DateFormatError(f'{MROIColumns.date} has incorrect date format. Expected format is yyyy-mm-dd.') from e
        self.logger.info('Date format check passed.')

    def drop_duplicates(self):
        """
        Drop duplicate rows in df
        """
        # Calculate total number of duplicates
        duplicates_count = self.dataframe.duplicated().sum()
        if duplicates_count > 0:
            self.logger.warn(f'Found {duplicates_count} duplicates in input data.')
            # Drop duplicates
            self.dataframe = self.dataframe.drop_duplicates()
            self.logger.info(f'Dropped {duplicates_count} duplicates.')
        self.logger.info('No duplicates found.')

    def handle_na(self):
        """
        Test for null values and backfill. Log warning for null values and na

        Raises:
            TooManyDateNullError: Raised when more than half of the rows in input file are null in date column
        """
        # Add datetime format to week_id using the output format from DE
        self.dataframe.replace([np.inf, -np.inf], np.nan)
        total_rows = self.dataframe.shape[0]
        # For each column in dataframe
        self.dataframe = self.dataframe.sort_values(by=MROIColumns.date)
        for column in self.dataframe.columns:
            nans_in_column = self.dataframe[column].isnull().sum()
            # Check if number of nans > 0
            if nans_in_column > 0 and (column not in [MROIColumns.date, MROIColumns.region]):
                self.logger.warn(f'Found {nans_in_column} null values in column: {column}.')
                # Backfill nulls first then forward fill
                self.dataframe[column]=self.dataframe[column].groupby(self.dataframe[MROIColumns.region]).transform(lambda x: x.bfill().ffill())
                self.logger.warn(f'Filled {nans_in_column} null values in column: {column}.')
            elif column == MROIColumns.region:
                region_null = nans_in_column
            elif column == MROIColumns.date:
                date_null = nans_in_column
                # Check if 50% of date values are nulls
                if nans_in_column / total_rows > 0.5:
                    # Raise exception
                    raise TooManyDateNullError(
                        f'Too many null values in date column. Found {nans_in_column} nulls out of {total_rows} rows. Possibly due to poor data quality. Please check input data sources.')
        if date_null > 0 or region_null > 0:
            # Drop all data null rows
            self.dataframe.dropna(inplace=True)
            self.logger.warn(f'Found {date_null+region_null} null values in date and region columns.')
            self.logger.warn(f'Dropped {date_null+region_null} rows corresponding to null values in date and region columns.')

    def check_data_consistency(self):
        """
        Test if dtypes are as expected and check if delta_lag is consistent
        """
        if self.dataframe[self.VOL_COL].dtype not in ['int', 'float']:
            # Try to set to int if not
            self.dataframe[self.VOL_COL] = self.dataframe[self.VOL_COL].astype('float')
            self.logger.info('Data type consistency check passed.')
        self.dataframe[MROIColumns.region] = self.dataframe[MROIColumns.region].astype('string').str.upper()
        
    # Find Minimum Volatility
    def find_min_volatility(self) -> dict:
        """
        Finds minimum volatility (standard deviation) for each region in data. Uses the dataframe initialized in class.

        Returns:
            dict: Dictionary with volatility for each region
        """
        j = {}
        self.logger.info('Calculating volatility (standard deviation) for each region.')
        for region in self.dataframe[MROIColumns.region].drop_duplicates():
            # Get data for region
            df_filter = self.dataframe.loc[self.dataframe[MROIColumns.region] == region]
            # Calculate standard deviation
            if df_filter.shape[0] > 1:
                std = stdev(df_filter[self.VOL_COL])
            else:
                self.logger.warn(f'Only one record for {region}. Setting volatility to 0.')
                std = 0
            # Add to dictionary
            j.update({region: std})
        self.logger.info('Calculated volatility (standard deviation) for each region.')
        return j

    def perform_diff_diff(self, region: str, is_test=1) -> dict:
        """
        Performs diff and diff computations. Sum of sales before and after campaign based on inputs to function.

        Args:
            region (str): region to calculate the sum amount
            is_test (int, optional): flag representing if region is a test region or not. Defaults to 1 (region is a test region). Defaults to 1.

        Returns:
            dict: dictionary containint pre and post sales for given region
        """
        self.logger.info(f'Calculating for {region} with is_test: {is_test}')
        # Retrieve data corresponding to the region
        self.dataframe[MROIColumns.region] = self.dataframe[MROIColumns.region].str.upper()
        df_proc = self.dataframe.loc[(self.dataframe[MROIColumns.region] == region)]
        # Slicing data frame to separate pre and post
        df_proc_pre = df_proc.loc[str(self.PRE_BEGIN):str(self.PRE_END)]
        df_proc_post = df_proc.loc[str(self.POST_BEGIN):str(self.POST_END)]
        # Calculate sum of amount for pre and post
        df_proc_pre_sum = df_proc_pre[self.VOL_COL].sum()
        df_proc_post_sum = df_proc_post[self.VOL_COL].sum()
        self.logger.info(f'Finished calculations for {region} with is_test: {is_test}')
        # Return the calculated fields to be added to output
        return {
            'REGION': region,
            'SALES_BEFORE': df_proc_pre_sum,
            'SALES_AFTER': df_proc_post_sum,
            'TEST_REGION_FLAG': is_test
        }

    def create_output(self) -> pd.DataFrame:
        """
        Creates output dataframe. Loops through each region provided in control and test region lists
        and calculates the required metrics.

        Returns:
            pd.DataFrame: output dataframe containing diff and diff metrics for all regions
        """
        output = []
        self.logger.info('Calculating and appending data for all regions in test and control regions to output dataframe.')
        for region in self.control_regions + self.test_regions:
            if region in self.control_regions:
                # If region is test, test_flag = 0
                test_flag = 0
            elif region in self.test_regions:
                # If region is test, test_flag = 1
                test_flag = 1
            else:
                self.logger.warn(f'''Could not find region: {region} in control or test region list provided in config. 
                                Skipping calculation.''')
                continue
            # Calculate diff diff fields for each region
            output.append(self.perform_diff_diff(region, is_test=test_flag))
            # self.logger.info(f'Appended data for region: {region} to output dataframe.')
        self.logger.info('Appended data for all regions in test and control regions to output dataframe.')
        # Create a dataframe from the list
        output_df = pd.DataFrame(output).sort_values(['TEST_REGION_FLAG', 'REGION'], ascending=False)
        output_df['SALES_BEFORE'] = output_df['SALES_BEFORE'].astype(float)
        output_df['SALES_AFTER'] = output_df['SALES_AFTER'].astype(float)
        return output_df

    def upload_results_gbq(self, df: pd.DataFrame):
        """
        Upload pivot table results to GBQ

        Args:
            df (pd.DataFrame): output table results
        """
        table_id = f"AIDE_Results.Diff_and_Diff" 
        self.logger.info(f"Uploading results to {self.PROJECT_ID}.{table_id}")
        columns = [re.sub('[^A-Za-z0-9_]+', '_', x) for x in df.columns]
        df.columns = columns
        df['run_id'] = f"{self.CONFIG['metadata']['aide_id']}"
        df['country'] = self.CONFIG['config']['country']
        df['country_code'] = self.CONFIG['config']['country_code']
        df['brand'] = self.CONFIG['config']['brand']
        df['sub_brand'] = ", ".join(self.CONFIG['config']['sub_brand'])
        df['segment'] = ", ".join(self.CONFIG['config']['segment'])
        df['sub_segment'] = ", ".join(self.CONFIG['config']['sub_segment'])
        df['granularity'] = self.MODEL_CONFIG['region_granularity']['type']
        df['pre_campaign_start'] = str(self.PRE_BEGIN)
        df['pre_campaign_end'] = str(self.PRE_END)
        df['campaign_start'] = str(self.POST_BEGIN)
        df['campaign_end'] = str(self.POST_END)
        df['test_group'] = f"test_{self.test_group}"
        df = df[['run_id', 'country', 'country_code', 'brand', 'sub_brand', 'segment', 'sub_segment', 'granularity', 'pre_campaign_start', 'pre_campaign_end', 'campaign_start', 'campaign_end', 'test_group'] + columns]
        df.to_gbq(table_id, project_id=self.PROJECT_ID, if_exists="append", credentials=google.auth.default()[0])
        self.logger.info("Loaded {} rows and {} columns to {}".format(df.shape[0], len(df.columns), table_id))
        
    def main(self):
        """
        Orchestrator method to execute all required functions in diff and diff class
        """
        self.dataframe = self.dataframe[[MROIColumns.date, MROIColumns.region, self.VOL_COL]]
        self.check_columns_headers()
        self.check_date_format()
        self.drop_duplicates()
        self.handle_na()
        self.check_data_consistency()
        self.logger.info(f'Reading campaign info')
        self.get_config()
        self.logger.info(f'Read campaign info successfully')
        self.dataframe = self.dataframe.set_index(MROIColumns.date)
        # region_volatility = self.find_min_volatility()
        output_df = self.create_output()
        self.logger.info(f'Uploading output file to {self.output_file_url}')
        output_df.to_csv(self.output_file_url.url, index=False)
        self.upload_results_gbq(output_df)
        self.logger.info(f'Uploaded output file successfully to {self.output_file_url}')

def delete_rows_bq(config: dict, model_config: dict):
    """
    Delete rows on BQ results table for model corresponding to previous run using the same run_id.

    Args:
        config (dict): config json generated by UI
    """
    run_id = config["metadata"]["aide_id"]
    country_code = config["config"]["country_code"]
    granularity = model_config["region_granularity"]["type"]
    query = f"""DELETE FROM `AIDE_Results.Diff_and_Diff`
        WHERE run_id='{run_id}' 
        AND country_code='{country_code}' 
        AND granularity='{granularity}' 
        """
    bigquery_client = bigquery.Client()
    #project_id = config['pipeline_metadata']['project_id']
    job = bigquery_client.query(query, job_id_prefix="delete_rows_diff_diff_previous_run", project=PROJECT_ID)
    job.result()
    main_logger.info(f'Successfully deleted older diff&diff results for previous run if exists with run_id: {run_id}, country_code: {country_code}, granularity: {granularity}')

if __name__ == "__main__":
    """
    Executes Diff and Diff model for all test groups
    """
    main_logger = getLogger(__name__)
    try:
        ### Start Execution
        start = datetime.now()
        country_code = json_config['config']['country_code']
        base_path = GCSUrl(EXECUTION_BASE_PATH)
        bucket = base_path.bucket
        gcs_output_path = f'{base_path.url}/outputs/DIFF&DIFF'
        
        main_logger.info(f'Starting diff&diff model execution now')
        ### Get pre-processing dataframes for each model to execute ###
        for model in json_config['models']:
            if model['id']=='DIFF&DIFF':
                run_diff_diff = model['config'].get('run_model', True)
                if run_diff_diff:
                    model_config = model['config']
        
        ### Execute Model
        if run_diff_diff:
            delete_rows_bq(json_config, model_config)
            total_test_cells = len(model_config['region_granularity']['test_control_regions']['test'])
            for test_group in range(total_test_cells):
                main_logger.info(f"Executing model for test group: {test_group}")
                input_file_path = f'{base_path.url}/inputs/DIFF&DIFF/DIFF&DIFF_{test_group}.csv'
                diff_diff = DiffDiff(PROJECT_ID, bucket, input_file_path, json_config, model_config, gcs_output_path, test_group)
                diff_diff.main()
                main_logger.info(f"Successfully executed model for test cell: {test_group}")
        
        ### Finish Execution
        end = datetime.now()
        main_logger.info('Start time:' + start.strftime('%c'))
        main_logger.info('End time:' + end.strftime('%c'))
        totalSeconds = (end - start).total_seconds()
        main_logger.info(f'Script took {timedelta(seconds=totalSeconds)} seconds to run')
    except (CampaignInfoError, ColumnHeadersError, DateFormatError, TooManyDateNullError, ValueError) as ce:
        main_logger.error(str(ce))
        main_logger.error(traceback.format_exc())
        default_notification_handler.send_failure_email(str(ce))
        default_jobstatus_handler.update_jobstatus(message=str(ce))
        raise
    
    ## general exception block that is not raised by above
    except Exception as e:
        main_logger.error(str(e))
        main_logger.error(traceback.format_exc())
        default_notification_handler.send_failure_email(f'An unexpected exception occurred: {str(e)}')
        default_jobstatus_handler.update_jobstatus(message='An unexpected exception occurred')
        raise
