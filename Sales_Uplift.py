"""Executes entire Sasles Uplift model"""
import argparse
import json
import sys
import re
from datetime import date, datetime, timedelta
from xmlrpc.client import boolean
import numpy as np
import pandas as pd
import traceback
from ntpath import basename
import os
from pyspark.sql import SparkSession

import google.auth
from google.cloud import storage
from google.cloud import bigquery
from google.cloud.exceptions import NotFound

from salesuplift_dependencies.sales_uplift import SalesUplift
from salesuplift_dependencies.exceptions import (
    AuthenticationError, ColumnHeadersError, DateFormatError, DownloadError, IncompatiblePythonVersionError, IncorrectPilotConfigurationError, InputParameterError, TooManyDateNullError
)

from mroi.config import PROJECT_ID, EXECUTION_BASE_PATH, json_config
from mroi.utils import gcs_join, GCSUrl
from mroi.notification import default_notification_handler, default_jobstatus_handler
from mroi.logging import getLogger

spark = SparkSession.builder.getOrCreate() # New BQ Table (20220311)-MS


def delete_rows_bq(config: dict, model_config: dict):
    """
    Delete rows on BQ results table for model corresponding to previous run using the same run_id.

    Args:
        config (dict): config json generated by UI
    """
    run_id = config["metadata"]["aide_id"]
    country_code = config["config"]["country_code"]
    granularity = model_config["region_granularity"]["type"]
    query = f"""DELETE FROM `AIDE_Results.Sales_Uplift`
        WHERE run_id='{run_id}' 
        AND country_code='{country_code}' 
        AND granularity='{granularity}' 
        """
    bigquery_client = bigquery.Client()
    # project_id = config['pipeline_metadata']['project_id']
    job = bigquery_client.query(query, job_id_prefix="delete_rows_rbt_previous_run", project=PROJECT_ID)
    job.result()
    main_logger.info(f'Successfully deleted older Sales Uplift results for previous run if exists with run_id: {run_id}, country_code: {country_code}, granularity: {granularity}')

def upload_results_gbq(df: pd.DataFrame, config: dict, Weekly: boolean):
    """
    Upload results for all test cells to GBQ
    
    Args:
        df (pd.DataFrame): concatenated output table results
        config (dict): config json generated by UI
    """
    if Weekly == False:
        table_id = f"AIDE_Results.Sales_Uplift"
    else:
        table_id = f"AIDE_Results.Sales_Uplift_Weekly"
    main_logger.info(f"Uploading results to {PROJECT_ID}.{table_id}")
    columns = [re.sub('[^A-Za-z0-9_]+', '_', col) for col in df.columns]
    df.columns = columns
    columns = [col for col in columns if col not in ['granularity', 'test_group', 'test_regions', 'control_regions']]
    df['run_id'] = f"{config['metadata']['aide_id']}"
    df['country'] = config['config']['country']
    df['country_code'] = config['config']['country_code']
    df['brand'] = config['config']['brand']
    df['sub_brand'] = ", ".join(config['config']['sub_brand'])
    df['segment'] = ", ".join(config['config']['segment'])
    df['sub_segment'] = ", ".join(config['config']['sub_segment'])
    df[['brand', 'sub_brand', 'segment', 'sub_segment', 'granularity', 'test_regions', 'control_regions']] = df[['brand', 'sub_brand', 'segment', 'sub_segment', 'granularity', 'test_regions', 'control_regions']].fillna('')
    df = df[['run_id', 'country', 'country_code', 'brand', 'sub_brand', 'segment', 'sub_segment', 'granularity', 'test_group', 'test_regions', 'control_regions'] + columns]
    df.to_gbq(table_id, project_id=PROJECT_ID, if_exists="append", credentials=google.auth.default()[0])
    main_logger.info("Loaded {} rows and {} columns to {}".format(df.shape[0], len(df.columns), table_id))
    
def validate_pilot_period(base_path: GCSUrl, test_group: str, model_config: dict):
    """
    Validates the data for pilot period specified in the config
    
    Args:
        base_path (GCSUrl): execution base path
        test_group (str): test cell/group id 
        model_config (dict): model config to read test cell specific parameters
    """
    input_df = pd.read_csv(gcs_join(base_path.url, f'inputs/SALES_UPLIFT/SALES_UPLIFT_{test_group}.csv'))
    try:
        # Try setting Week_id format to '%Y-%m-%d' which is expected
        input_df['Week_id'] = pd.to_datetime(input_df['Week_id'], format='%Y-%m-%d', errors='raise')
    except Exception:
        raise DateFormatError('Week_id has incorrect date format. Expected format is yyyy-mm-dd.')
        
    try:
        input_combinations = pd.DataFrame.from_dict([model_config['region_granularity']['test_control_regions']['test'][str(test_group)]['test_group_parameters']])
        input_combinations.columns = [x.upper() for x in input_combinations.columns]
        input_combinations = input_combinations[['PILOT_START', 'WEEKS_PILOT', 'INCREMENTAL_SPEND', 'MIN_CI']]
    except AttributeError:
        raise InputParameterError(f'Expected one value each for PILOT_START, WEEKS_PILOT, INCREMENTAL_SPEND. '
                    f'Received {len(input_combinations.columns)} values.')
    input_combinations['PILOT_START'] = pd.to_datetime(input_combinations['PILOT_START'], format='%Y-%m-%d', errors='raise')
    # check if monthly or weekly
    if any(input_df.groupby(by=[input_df.Week_id.dt.month, input_df.Week_id.dt.year]).count().iloc[:, 0].values > 1): # weekly
        data_weekday = input_df['Week_id'][0].weekday()
        config_weekday = input_combinations['PILOT_START'].dt.weekday[0]
        if data_weekday > config_weekday:
            input_combinations['PILOT_START'] = input_combinations['PILOT_START'] - pd.to_timedelta(7-(data_weekday-config_weekday), unit='D')
        elif config_weekday > data_weekday:
            input_combinations['PILOT_START'] = input_combinations['PILOT_START'] - pd.to_timedelta(config_weekday-data_weekday, unit='D')
    else: # monthly
        input_combinations['PILOT_START'] = input_combinations['PILOT_START'] - pd.to_timedelta(input_combinations['PILOT_START'].dt.day-1, unit='D')
    
    input_combinations['WEEKS_PILOT'] = input_combinations['WEEKS_PILOT'].astype(int)
    pilot_end = input_combinations['PILOT_START'] + pd.to_timedelta(input_combinations['WEEKS_PILOT'], unit='W')
    input_df['set'] = np.select([input_df['Week_id'] < input_combinations['PILOT_START'][0], 
        input_df['Week_id'] < pilot_end[0], input_df['Week_id'] >= pilot_end[0]], 
        ['train', 'pilot', 'undefined']) 
    if input_df[input_df['set']=='pilot'].shape[0] == input_combinations['WEEKS_PILOT'][0]:
        main_logger.info(f"Data check for pilot period successful for test cell: {test_group}.")
        return
    else:
        raise IncorrectPilotConfigurationError(f"""Missing data for the specified pilot period with pilot start: {input_combinations['PILOT_START'][0]} 
        and weeks pilot : {input_combinations['WEEKS_PILOT'][0]} for test cell: {test_group}.""")
        
if __name__ == "__main__":
    """
    Executes Sales Uplift model
    """
    main_logger = getLogger(__name__)
    try:
        # ## Start Execution
        start = datetime.now()
        country_code = json_config['config']['country_code']
        base_path = GCSUrl(EXECUTION_BASE_PATH)
        #project_id = payload['pipeline_metadata']['project_id']
        # ## Prepare Execution
        
        main_logger.info(f'Starting Sales Uplift model execution now')
        for model in json_config['models']:
            if model['id']=='SALES_UPLIFT':
                run_sales_uplift = model['config'].get('run_model', True)
                if run_sales_uplift:
                    model_config = model['config']
        
        # This is only required by Sales Uplift model
        if run_sales_uplift:
            delete_rows_bq(json_config, model_config)
            total_test_groups = len(model_config['region_granularity']['test_control_regions']['test'])
            models_used = {}
            sales_uplift_obj = {}
            combined_output_dict = {}
            BQ_combined_output_dict = {} # New BQ Table (20220311)-MS
            for test_group in range(total_test_groups):
                validate_pilot_period(base_path, test_group, model_config)
            for test_group in range(total_test_groups):
                main_logger.info(f"Executing model for test group: {test_group}")
                input_file_path = gcs_join(base_path.url, f'inputs/SALES_UPLIFT/SALES_UPLIFT_{test_group}.csv')
                model_intermediate_path = gcs_join(base_path.url, f'intermediate-files/SALES_UPLIFT/test_{test_group}')
                model_output_path = gcs_join(base_path.url, f'outputs/SALES_UPLIFT/test_{test_group}')
                sales_uplift_obj[test_group] = SalesUplift(PROJECT_ID, input_file_path, model_intermediate_path, model_output_path, json_config, model_config, test_group, models_used)
                test_cell_output, models_used, BQFinalOutput = sales_uplift_obj[test_group].main(only_ts=False)
                combined_output_dict[test_group] = test_cell_output
                BQ_combined_output_dict[test_group] = BQFinalOutput # New BQ Table (20220311)-MS
                main_logger.info(f"Successfully executed model for test group: {test_group}")
                
            if len(list(set(list(models_used.values())))) != 1:
                main_logger.info("Different models used for test cells, executing Time series model for the test cells with GLM results.")
                for test_group in range(total_test_groups):
                    if models_used[test_group] != 'ts':
                        main_logger.info(f"Executing Time series model for test group: {test_group}")
                        try:
                            main_logger.info(f"Checking if results for previous execution of Time series model for test group: {test_group} exist.")
                            test_cell_output = pd.read_csv(gcs_join(sales_uplift_obj[test_group].model_intermediate_path, f'final_output_ts.csv'))
                            test_cell_output.to_csv(gcs_join(sales_uplift_obj[test_group].model_output_path, f'final_output.csv'), index=False)
                            test_cell_model_predictions = pd.read_csv(gcs_join(sales_uplift_obj[test_group].model_intermediate_path, f'pilot_final_predictions_ts.csv'))
                            test_cell_model_predictions.to_csv(gcs_join(sales_uplift_obj[test_group].model_output_path, f'pilot_final_predictions.csv'), index=False)
                        # New BQ Table Begin (20220311)-MS
                            spk_final_output = pd.read_csv(gcs_join(sales_uplift_obj[test_group].model_output_path, f'final_output.csv'))
                            spark_final_output = spark.createDataFrame(spk_final_output.astype(str))
                            spk_pilot_final_predictions = pd.read_csv(gcs_join(sales_uplift_obj[test_group].model_output_path, f'pilot_final_predictions.csv'))
                            spark_pilot_final_predictions = spark.createDataFrame(spk_pilot_final_predictions.astype(str))
                            BQFinalOutput = (spark_pilot_final_predictions
                                             .select("Week_id", "actual_sales", "predicted_sales")
                                             .crossJoin(spark_final_output)
                                            ).toPandas()
                            BQFinalOutput.to_csv(gcs_join(sales_uplift_obj[test_group].model_output_path, f'BQ_final_output.csv'), index=False)
                        # New BQ Table End (20220311)-MS
                            models_used[test_group] = 'ts'
                            main_logger.info(f"Obtained results for previous execution of Time series model for test group: {test_group}.")
                        except FileNotFoundError:
                            main_logger.info(f"Couldn't locate results for previous execution of Time series model for test group: {test_group}. Executing Time series model.")
                            test_cell_output, models_used = sales_uplift_obj[test_group].main(only_ts=True)
                            main_logger.info(f"Successfully executed Time series model for test group: {test_group}.")
                        combined_output_dict[test_group] = test_cell_output
                        BQ_combined_output_dict[test_group] = BQFinalOutput # New BQ Table (20220311)-MS
                        
            combined_output = pd.concat(combined_output_dict.values())
            combined_output.to_csv(model_output_path.replace(basename(model_output_path), 'SALES_UPLIFT_all.csv'), index=False, mode='w', header=True)
            upload_results_gbq(combined_output, json_config, False)
         # New BQ Table (20220311)-MS
            BQ_combined_output = pd.concat(BQ_combined_output_dict.values())
            BQ_combined_output.to_csv(model_output_path.replace(basename(model_output_path), 'BQFinalOutput.csv'), index=False, mode='w', header=True)
            upload_results_gbq(BQ_combined_output, json_config, True)

        # ## Finish Execution
        end = datetime.now()
        main_logger.info('Start time:' + start.strftime('%c'))
        main_logger.info('End time:' + end.strftime('%c'))
        totalSeconds = (end - start).total_seconds()
        main_logger.info(f'Script took {timedelta(seconds=totalSeconds)} seconds to run')
    except (AuthenticationError, ColumnHeadersError, DateFormatError, DownloadError, IncompatiblePythonVersionError, IncorrectPilotConfigurationError, InputParameterError, TooManyDateNullError) as ce:
        main_logger.error(str(ce))
        main_logger.error(traceback.format_exc())
        default_notification_handler.send_failure_email(str(ce))
        default_jobstatus_handler.update_jobstatus(message=str(ce))
        raise
    
    ## general exception block that is not raised by above
    except Exception as e:
        main_logger.error(str(e))
        main_logger.error(traceback.format_exc())
        default_notification_handler.send_failure_email(f'An unexpected exception occurred: {str(e)}')
        default_jobstatus_handler.update_jobstatus(message='An unexpected exception occurred')
        raise
